## 测试环境
所有的测试都是在配置8张TITAN V-12066MiB GPU的服务器中进行，主要硬软件配置如下：
- `nvidia-smi topo -m`  
```
	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	CPU Affinity
GPU0	 X 	PIX	PHB	PHB	SYS	SYS	SYS	SYS	0-13,28-41
GPU1	PIX	 X 	PHB	PHB	SYS	SYS	SYS	SYS	0-13,28-41
GPU2	PHB	PHB	 X 	PIX	SYS	SYS	SYS	SYS	0-13,28-41
GPU3	PHB	PHB	PIX	 X 	SYS	SYS	SYS	SYS	0-13,28-41
GPU4	SYS	SYS	SYS	SYS	 X 	PIX	PHB	PHB	14-27,42-55
GPU5	SYS	SYS	SYS	SYS	PIX	 X 	PHB	PHB	14-27,42-55
GPU6	SYS	SYS	SYS	SYS	PHB	PHB	 X 	PIX	14-27,42-55
GPU7	SYS	SYS	SYS	SYS	PHB	PHB	PIX	 X 	14-27,42-55

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
```
- ### OneFlow BERT
  - OneFlow: [master@c8695c0c3768fd3d154c22fa1c997907e4b46666](https://github.com/Oneflow-Inc/oneflow)
  - OneFlow-Benchmark: [master@854ddd06b49dfd43b1233ef7f0800947ba633bb2](https://github.com/Oneflow-Inc/OneFlow-Benchmark)

- `git clone https://github.com/NVIDIA/DeepLearningExamples`
- `cd DeepLearningExamples/TensorFlow/LanguageModeling/BERT/`
- 直接使用已有镜像，`docker load -i /DATA/disk1/of_output/nvidia_bert_tf_backup.tar.gz`
- 启动容器 根据构建好的项目镜像启动容器，在DeepLearningExamples/TensorFlow/LanguageModeling/BERT/下运行：bash scripts/docker/launch.sh ，修改launch.sh，为容器提供必要的启动参数：
```
# 启动容器
#!/bin/bash

CMD=${@:-/bin/bash}
NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-"all"}

nvidia-docker run  -it \
    --net=host --shm-size=16g \
    --ulimit memlock=-1 --privileged \
    --name tf_bert \
    --ulimit stack=67108864 \
    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \
    -v $PWD:/workspace/bert \
    -v /DATA/disk1/bert/tf_wiki_seq_len_128:/workspace/bert/data/tfrecord \
    -v $PWD/results:/results \
    nvidia_bert_tf:20.03 $CMD
```
- ctrl + p + q 退出容器，在DeepLearningExamples/TensorFlow/LanguageModeling/BERT/data目录下设置`bert_config.json`
```
{
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}
```
- 停止容器，可再`docker start 292`,292为容器id前三位。`docker attach 292`连接
- 修改scripts/run_pretraining_adam.sh，可参考[这个](https://github.com/Oneflow-Inc/DLPerf/blob/master/NVIDIADeepLearningExamples/TensorFlow/LanguageModeling/BERT/scripts/run_pretraining_adam.sh).主要修改：`train_steps`为1000000，`--learning_rate=1.25e-5`
```
rm -rf /results/*
echo "Container nvidia build = " $NVIDIA_BUILD_ID

DATA_DIR=${1:-"data"}
num_gpus=${2:-8}
train_batch_size=${3:-32}
train_steps=${4:-1000000}
bert_model="base"
precision=${5:-"fp32"}
use_xla=${6:-"false"}
max_pred_per_seq=20
seq_len=128
TEST_NUM=${7:-1}
num_accumulation_steps=1

# DATA_DIR=data/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus

if [ "$bert_model" = "large" ] ; then
    export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json
else
    # export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json
    export BERT_CONFIG=data/bert_config.json
fi

PREC=""
if [ "$precision" = "fp16" ] ; then
   PREC="--use_fp16"
elif [ "$precision" = "fp32" ] ; then
   PREC=""
elif [ "$precision" = "manual_fp16" ] ; then
   PREC="--manual_fp16"
else
   echo "Unknown <precision> argument"
   exit -2
fi

if [ "$use_xla" = "true" ] ; then
    PREC="$PREC --use_xla"
    echo "XLA activated"
fi

export GBS=$(expr $train_batch_size \* $num_gpus \* $num_accumulation_steps)
printf -v TAG "tf_bert_pretraining_adam_%s_%s_gbs%d" "$bert_model" "$precision" $GBS
DATESTAMP=`date +'%y%m%d%H%M%S'`

#Edit to save logs & checkpoints in a different directory
RESULTS_DIR=${RESULTS_DIR:-/results/${TAG}_${DATESTAMP}}
LOG_FOLDER=./logs/ngc/tensorflow/bert/bz${train_batch_size}/1n${num_gpus}g
mkdir -p $LOG_FOLDER
LOGFILE=${LOG_FOLDER}/bert_b${train_batch_size}_${precision}_$TEST_NUM.log
mkdir -m 777 -p $RESULTS_DIR
printf "Saving checkpoints to %s\n" "$RESULTS_DIR"
printf "Logs written to %s\n" "$LOGFILE"

INPUT_FILES="$DATA_DIR/tfrecord"
EVAL_FILES="$DATA_DIR/tfrecord"

horovod_str=""
mpi=""
if [ $num_gpus -gt 1 ] ; then
   mpi="mpiexec --allow-run-as-root -np $num_gpus --bind-to socket"
   horovod_str="--horovod"
fi

CMD="$mpi python3 /workspace/bert/run_pretraining.py"
CMD+=" --input_files_dir=$INPUT_FILES"
CMD+=" --eval_files_dir=$EVAL_FILES"
CMD+=" --output_dir=$RESULTS_DIR"
CMD+=" --bert_config_file=$BERT_CONFIG"
CMD+=" --do_train=True"
CMD+=" --do_eval=False"
CMD+=" --train_batch_size=$train_batch_size"
CMD+=" --eval_batch_size=16"
CMD+=" --max_seq_length=$seq_len"
CMD+=" --max_predictions_per_seq=$max_pred_per_seq"
CMD+=" --num_train_steps=$train_steps"
CMD+=" --num_warmup_steps=10000"
CMD+=" --num_accumulation_steps=$num_accumulation_steps"
CMD+=" --save_checkpoints_steps=10000"
CMD+=" --learning_rate=1.25e-5"
CMD+=" --optimizer_type=adam"
CMD+=" $horovod_str $PREC"
CMD+=" --allreduce_post_accumulation=False"

#Check if all necessary files are available before training
for DIR_or_file in $DATA_DIR $BERT_CONFIG $RESULTS_DIR; do
  if [ ! -d "$DIR_or_file" ] && [ ! -f "$DIR_or_file" ]; then
     echo "Error! $DIR_or_file directory missing. Please mount correctly"
     exit -1
  fi
done


$CMD   2>&1 | tee ${LOGFILE}

echo "Writting log to ${LOGFILE}"
```
- `vim run_pretraining.py`修改display_loss_steps为100打印一次
- 还需要修改amp参数默认为false,最新master开启loss_scale由两个参数控制
- 直接 bash scripts/run_pretraining_adam.sh就可以训练起来
- 注意
  - `run_pretraining_adam.sh`里的学习率是每卡的学习率，具体请参考[这里](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/run_pretraining.py#L604)和[这里](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/scripts/run_pretraining_adam.sh#L77)
  - 